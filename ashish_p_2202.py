# -*- coding: utf-8 -*-
"""Ashish/p/2202

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JN5B2e7oWsW-rqepTfNbHnw6PD7IrG9t

**1Ô∏è‚É£ SPARK SESSION CONFIGURATION (RESOURCE OPTIMIZED)**
"""

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Coventry_UK_Collision_Distributed_Project") \
    .config("spark.executor.instances", "4") \
    .config("spark.executor.cores", "4") \
    .config("spark.executor.memory", "6g") \
    .config("spark.driver.memory", "4g") \
    .config("spark.sql.shuffle.partitions", "400") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

"""**2Ô∏è‚É£ DATA INGESTION + VALIDATION**"""

from pyspark.sql.functions import expr

raw_df = spark.read \
    .option("header", "true") \
    .option("mode", "PERMISSIVE") \
    .csv("/content/dft-road-casualty-statistics-collision-1979-latest-published-year.csv")

# Safe casting using try_cast
df = raw_df.select(
    expr("try_cast(collision_year as int) as collision_year"),
    expr("try_cast(collision_severity as double) as collision_severity"),
    expr("try_cast(number_of_vehicles as int) as number_of_vehicles"),
    expr("try_cast(number_of_casualties as int) as number_of_casualties"),
    expr("try_cast(speed_limit as int) as speed_limit"),
    expr("try_cast(weather_conditions as int) as weather_conditions"),
    expr("try_cast(road_surface_conditions as int) as road_surface_conditions"),
    expr("try_cast(light_conditions as int) as light_conditions"),
    "time"
)

df = df.dropna(subset=["collision_severity"])

"""**3Ô∏è‚É£ PARTITIONING + PARQUET STORAGE**"""

df = df.repartition("collision_year")

df.write \
  .partitionBy("collision_year") \
  .mode("overwrite") \
  .parquet("collision_parquet")

df = spark.read.parquet("collision_parquet")

"""**4Ô∏è‚É£ CUSTOM TRANSFORMER (DOMAIN FEATURE ENGINEERING)**"""

from pyspark.ml import Transformer
from pyspark.sql.functions import hour, to_timestamp

class TimeFeatureTransformer(Transformer):
    def _transform(self, dataset):
        dataset = dataset.withColumn(
            "parsed_time",
            to_timestamp("time", "HH:mm")
        )
        return dataset.withColumn(
            "hour_of_day",
            hour("parsed_time")
        )

time_transformer = TimeFeatureTransformer()
df = time_transformer.transform(df)
df = df.fillna(0)

"""**5Ô∏è‚É£ CACHING STRATEGY**"""

df.persist()
print("Total Rows:", df.count())

"""**6Ô∏è‚É£ VECTOR ASSEMBLY**"""

from pyspark.ml.feature import VectorAssembler

feature_cols = [
    "collision_year",
    "number_of_vehicles",
    "number_of_casualties",
    "speed_limit",
    "weather_conditions",
    "road_surface_conditions",
    "light_conditions",
    "hour_of_day"
]

assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol="features"
)

data = assembler.transform(df) \
    .select("features", df.collision_severity.alias("label"))

"""**7Ô∏è‚É£ TRAIN TEST SPLIT**"""

train, test = data.randomSplit([0.8, 0.2], seed=42)

"""**8Ô∏è‚É£ MLlib MODELS (4 Algorithms)**"""

from pyspark.ml.regression import (
    LinearRegression,
    DecisionTreeRegressor,
    RandomForestRegressor,
    GBTRegressor
)

from pyspark.ml.evaluation import RegressionEvaluator
import time

evaluator = RegressionEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="rmse"
)

# ---------------- Linear Regression ----------------
start = time.time()
lr = LinearRegression()
lr_model = lr.fit(train)
lr_time = time.time() - start
lr_rmse = evaluator.evaluate(lr_model.transform(test))

# ---------------- Decision Tree ----------------
start = time.time()
dt = DecisionTreeRegressor()
dt_model = dt.fit(train)
dt_time = time.time() - start
dt_rmse = evaluator.evaluate(dt_model.transform(test))

# ---------------- Random Forest ----------------
start = time.time()
rf = RandomForestRegressor(numTrees=50, maxDepth=10)
rf_model = rf.fit(train)
rf_time = time.time() - start
rf_rmse = evaluator.evaluate(rf_model.transform(test))

# ---------------- GBT ----------------
start = time.time()
gbt = GBTRegressor(maxIter=20)
gbt_model = gbt.fit(train)
gbt_time = time.time() - start
gbt_rmse = evaluator.evaluate(gbt_model.transform(test))

print("LR RMSE:", lr_rmse, "Time:", lr_time)
print("DT RMSE:", dt_rmse, "Time:", dt_time)
print("RF RMSE:", rf_rmse, "Time:", rf_time)
print("GBT RMSE:", gbt_rmse, "Time:", gbt_time)

"""**9Ô∏è‚É£ DISTRIBUTED CROSS VALIDATION**"""

from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

paramGrid = ParamGridBuilder() \
    .addGrid(rf.numTrees, [20, 50]) \
    .addGrid(rf.maxDepth, [5, 10]) \
    .build()

crossval = CrossValidator(
    estimator=rf,
    estimatorParamMaps=paramGrid,
    evaluator=evaluator,
    numFolds=3,
    parallelism=4
)

cv_model = crossval.fit(train)
best_rmse = evaluator.evaluate(cv_model.transform(test))

print("Best Tuned RF RMSE:", best_rmse)

"""**üîü MODEL SERIALIZATION**"""

from sklearn.ensemble import RandomForestRegressor
import pandas as pd

sample_pd = data.limit(50000).toPandas()

X = pd.DataFrame(sample_pd["features"].tolist())
y = sample_pd["label"]

import time
start = time.time()
rf_sklearn = RandomForestRegressor()
rf_sklearn.fit(X, y)
sk_time = time.time() - start

print("Sklearn RF Training Time:", sk_time)

print("Total Rows:", df.count())
print("Total Columns:", len(df.columns))

df.printSchema()

df.show(5)

from pyspark.sql.functions import col, sum

missing = df.select([
    sum(col(c).isNull().cast("int")).alias(c)
    for c in df.columns
])

missing.show()

df.groupBy("collision_severity") \
  .count() \
  .orderBy("collision_severity") \
  .show()

import matplotlib.pyplot as plt

severity_pd = df.groupBy("collision_severity") \
                .count() \
                .orderBy("collision_severity") \
                .toPandas()

plt.figure()
plt.bar(severity_pd["collision_severity"], severity_pd["count"])
plt.title("Collision Severity Distribution")
plt.xlabel("Severity (1=Fatal, 2=Serious, 3=Slight)")
plt.ylabel("Count")
plt.show()

year_trend = df.groupBy("collision_year") \
               .count() \
               .orderBy("collision_year")

year_trend.show()

year_pd = year_trend.toPandas()

plt.figure()
plt.plot(year_pd["collision_year"], year_pd["count"])
plt.title("Accidents per Year")
plt.xlabel("Year")
plt.ylabel("Number of Accidents")
plt.show()

df.groupBy("speed_limit") \
  .count() \
  .orderBy("speed_limit") \
  .show()

speed_pd = df.groupBy("speed_limit") \
             .count() \
             .orderBy("speed_limit") \
             .toPandas()

plt.figure()
plt.bar(speed_pd["speed_limit"], speed_pd["count"])
plt.title("Accidents by Speed Limit")
plt.xlabel("Speed Limit")
plt.ylabel("Count")
plt.show()

df.groupBy("weather_conditions") \
  .count() \
  .orderBy("weather_conditions") \
  .show()

df.groupBy("weather_conditions") \
  .count() \
  .orderBy("weather_conditions") \
  .show()

numeric_cols = [
    "collision_year",
    "number_of_vehicles",
    "number_of_casualties",
    "speed_limit"
]

for col_name in numeric_cols:
    print(f"Correlation with severity ({col_name}):",
          df.stat.corr(col_name, "collision_severity"))